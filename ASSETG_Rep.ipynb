{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "conn = wrds.Connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATAFMT='STD' and INDFMT='INDL' and CONSOL='C' and POPSRC='D' to retrieve the standardized (as opposed to re-stated data), consolidated (as opposed to pro-forma) data presented in the industrial format (as opposed to financial services format) for domestic companys (as opposed to international firms), i.e., the U.S. and Canadian firms.\n",
    "\n",
    "PRCC_C: close market price, CSHO: net number of all common shares\n",
    "\n",
    "Importing linking table to convert permno to gvkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSETG_query = \"\"\"\n",
    "SELECT gvkey, datadate, at\n",
    "FROM comp.funda\n",
    "WHERE indfmt='INDL' \n",
    "AND datafmt='STD' \n",
    "AND popsrc='D' \n",
    "AND consol='C' \n",
    "AND datadate >= '1961-12-31' \n",
    "AND datadate <= '2002-12-31'\n",
    "\"\"\"\n",
    "June_query = \"\"\"\n",
    "SELECT permno, date, shrout\n",
    "FROM crsp.msf\n",
    "WHERE date >= '1962-06-30' -- Adjusted start date to align with ASSETG data\n",
    "AND date <= '2003-06-30' -- Adjusted end date\n",
    "AND EXTRACT(MONTH FROM date) = 6 AND EXTRACT(DAY FROM date) = 30 -- Filter for June 30th\n",
    "\"\"\"\n",
    "\n",
    "Link_query = \"\"\"\n",
    "SELECT Gvkey, Lpermno, Linkdt, Linkenddt\n",
    "FROM crsp.ccmxpf_linktable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Calculates L1ASSETG & L2 ASSETG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_assetg(group):\n",
    "    # Shift within the group\n",
    "    group['at_lag3'] = group['at'].shift(3)  \n",
    "    group['at_lag2'] = group['at'].shift(2)\n",
    "    group['at_lag1'] = group['at'].shift(1)\n",
    "\n",
    "    # Calculate ASSETG\n",
    "    group['ASSETG'] = (group['at_lag1'] - group['at_lag2']) / group['at_lag2']\n",
    "    group['L2ASSETG'] = (group['at_lag2'] - group['at_lag3']) / group['at_lag3']\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab data\n",
    "ASSETG_data = conn.raw_sql(ASSETG_query)\n",
    "June_data = conn.raw_sql(June_query)\n",
    "daily_rets = pd.read_csv('daily_rets.csv')\n",
    "linking_table = conn.raw_sql(Link_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert closing price to abs, filter to only include june 30th entries, merge to shares outstanding df & calculate market value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_rets['prc'] = daily_rets['prc'].abs()\n",
    "daily_rets['date'] = pd.to_datetime(daily_rets['date'])\n",
    "daily_rets = daily_rets[(daily_rets['date'].dt.month == 6) & (daily_rets['date'].dt.day == 30)]\n",
    "\n",
    "June_data['date'] = pd.to_datetime(June_data['date'])\n",
    "merged_data = pd.merge(June_data, daily_rets[['permno', 'date', 'prc', 'ret']], \n",
    "                       on=['permno', 'date'], \n",
    "                       how='left')\n",
    "merged_data = merged_data.dropna()\n",
    "merged_data['MV'] = merged_data['shrout'] * merged_data['prc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creates year column, \n",
    "ensures proper sorting & filters by december month (redundent maybe),\n",
    "applies shifts and calculates ASSETG, drops na and 0 observations, creates deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSETG_data['datadate'] = pd.to_datetime(ASSETG_data['datadate'])\n",
    "ASSETG_data['year'] = ASSETG_data['datadate'].dt.year\n",
    "ASSETG_data = ASSETG_data.sort_values(['gvkey','datadate'])\n",
    "ASSETG_data = ASSETG_data[ASSETG_data['datadate'].dt.month==12]\n",
    "ASSETG_data = ASSETG_data.groupby('gvkey').apply(calculate_assetg)\n",
    "ASSETG_data = ASSETG_data[\n",
    "    (~pd.isna(ASSETG_data['at_lag1'])) & (ASSETG_data['at_lag1'] != 0) &\n",
    "    (~pd.isna(ASSETG_data['at_lag2'])) & (ASSETG_data['at_lag2'] != 0) &\n",
    "    (~pd.isna(ASSETG_data['at_lag3'])) & (ASSETG_data['at_lag3'] != 0)\n",
    "]\n",
    "ASSETG_data['decile'] = pd.qcut(ASSETG_data['ASSETG'], 10, labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging linking table to associate MV dates to ASSETG table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "linking_table = linking_table.dropna(subset=['lpermno'])\n",
    "linking_table['linkdt'] = pd.to_datetime(linking_table['linkdt'])\n",
    "linking_table['linkenddt'] = pd.to_datetime(linking_table['linkenddt'])\n",
    "\n",
    "ASSETG_data = ASSETG_data.reset_index(drop=True)\n",
    "ASSETG_data = ASSETG_data.merge(linking_table, on='gvkey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge ASSETG table with MV table with Permno & gvkey associations and add \"june\" date. Need to fix loss of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSETG_data = ASSETG_data[\n",
    "    (ASSETG_data['datadate'] >= ASSETG_data['linkdt']) & \n",
    "    (ASSETG_data['datadate'] <= ASSETG_data['linkenddt'])\n",
    "].copy()\n",
    "ASSETG_data['MV_date'] = ASSETG_data['datadate'] + pd.offsets.DateOffset(years=1)\n",
    "ASSETG_data['MV_date'] = ASSETG_data['MV_date'].apply(lambda x: x.replace(month=6, day=30))\n",
    "\n",
    "merged_data['date'] = pd.to_datetime(merged_data['date'])\n",
    "ASSETG_data = ASSETG_data.merge(\n",
    "    merged_data, \n",
    "    left_on=['lpermno', 'MV_date'], \n",
    "    right_on=['permno', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "ASSETG_data = ASSETG_data.dropna()\n",
    "ASSETG_data = ASSETG_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates yearly returns for June date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_121995/1262929355.py:3: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  ASSETG_data['yearly_return'] = ASSETG_data.groupby('gvkey')['prc'].pct_change(1)\n"
     ]
    }
   ],
   "source": [
    "ASSETG_data.set_index(['gvkey', 'date'], inplace=True)\n",
    "ASSETG_data.sort_index(inplace=True)\n",
    "ASSETG_data['yearly_return'] = ASSETG_data.groupby('gvkey')['prc'].pct_change(1)\n",
    "ASSETG_data = ASSETG_data.dropna()\n",
    "ASSETG_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creates dictionary with this strcuture: gvkey(year(ASSETG:x, decile:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = defaultdict(lambda: defaultdict(dict))\n",
    "for idx, row in ASSETG_data.iterrows():\n",
    "    data_dict[row['gvkey']][row['year']] = {'ASSETG': row['ASSETG'], 'L2ASSETG': row['L2ASSETG'], 'MV': row['MV'], 'decile': row['decile'], 'yearly_return': row['yearly_return']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each unique gvkey: checks that iterated year has valid period, grabs decile of iterated year and for each year surrounding it grabs associated ASSETG, sums it with associated decile and offset year key, counts the amount of times its iterated, calculates averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_complete_data_for_period(data_dict, gvkey, center_year):\n",
    "    for year_offset in range(-4, 6):  # 10-year window: 4 years back, 5 years forward\n",
    "        year = center_year + year_offset\n",
    "        if year not in data_dict[gvkey]:\n",
    "            return False  # Data for this year is missing or incomplete\n",
    "    return True  # All years in the period have the necessary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSETG decile Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Averaging Logic\n",
    "sum_data = defaultdict(lambda: defaultdict(int))\n",
    "count_data = defaultdict(lambda: defaultdict(int))\n",
    "average_data = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for gvkey in tqdm(ASSETG_data['gvkey'].unique(), desc='Processing stocks'):\n",
    "    min_year = min(data_dict[gvkey]) + 4\n",
    "    max_year = max(data_dict[gvkey]) - 5 \n",
    "    for center_year in tqdm(range(min_year, max_year + 1), desc=f'Processing years for gvkey {gvkey}'):\n",
    "        if has_complete_data_for_period(data_dict, gvkey, center_year):\n",
    "            center_year_decile = data_dict[gvkey][center_year]['decile']\n",
    "            for year_offset in range(-4, 6):\n",
    "                year = center_year + year_offset\n",
    "                if year in data_dict[gvkey]:\n",
    "                    assetg_value = data_dict[gvkey][year]['ASSETG']\n",
    "\n",
    "                    # Update sum and count\n",
    "                    sum_data[center_year_decile][year_offset] += assetg_value\n",
    "                    count_data[center_year_decile][year_offset] += 1\n",
    "\n",
    "                    # Calculate and update the average\n",
    "                    current_sum = sum_data[center_year_decile][year_offset]\n",
    "                    current_count = count_data[center_year_decile][year_offset]\n",
    "                    average_data[center_year_decile][year_offset] = current_sum / current_count if current_count != 0 else 0\n",
    "\n",
    "        else:\n",
    "            print(f\"Incomplete data for gvkey {gvkey} in center year {center_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates ASSETG decile panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_df = {}\n",
    "for year_offset in range(-4, 6):\n",
    "    row_data = []\n",
    "    for decile in range(10):\n",
    "            row_data.append(average_data[decile][year_offset])\n",
    "    data_for_df[year_offset] = row_data\n",
    "\n",
    "df = pd.DataFrame(data_for_df, index=range(10)).transpose()\n",
    "df.columns = [f'Decile {i}' for i in range(10)]\n",
    "df.index.name = 'Year Offset'\n",
    "df['9-0 Spread'] = df['Decile 9'] - df['Decile 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EW & VW Raw Return Portfolios NOT ENOUGH DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_market_cap = defaultdict(float)\n",
    "annual_weights = defaultdict(lambda: defaultdict(float))\n",
    "weighted_sum_data = defaultdict(lambda: defaultdict(float))\n",
    "weighted_count_data = defaultdict(lambda: defaultdict(float))\n",
    "value_weighted_average_data = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "equal_weight_sum_data = defaultdict(lambda: defaultdict(float))\n",
    "equal_weight_count_data = defaultdict(lambda: defaultdict(int))\n",
    "equal_weight_average_data = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "\n",
    "\n",
    "for gvkey, yearly_data in data_dict.items():\n",
    "    for year, attributes in yearly_data.items():\n",
    "        # Total market value for each year\n",
    "        total_market_cap[year] += attributes['MV']\n",
    "\n",
    "for gvkey, yearly_data in data_dict.items():\n",
    "    for year, attributes in yearly_data.items():\n",
    "        # Calculate the weight of each gvkey for each year\n",
    "        if total_market_cap[year] > 0:\n",
    "            annual_weights[year][gvkey] = attributes['MV'] / total_market_cap[year]\n",
    "\n",
    "for gvkey in tqdm(ASSETG_data['gvkey'].unique(), desc='Processing stocks'):\n",
    "    min_year = min(data_dict[gvkey]) + 4\n",
    "    max_year = max(data_dict[gvkey]) - 5\n",
    "\n",
    "    for center_year in range(min_year, max_year + 1):\n",
    "        if has_complete_data_for_period(data_dict, gvkey, center_year):\n",
    "            center_year_decile = data_dict[gvkey][center_year]['decile']\n",
    "            for year_offset in range(-4, 6):\n",
    "                year = center_year + year_offset\n",
    "                if year in data_dict[gvkey]:\n",
    "                    yearly_return_value = data_dict[gvkey][year]['yearly_return']\n",
    "                    weight = annual_weights[year][gvkey]\n",
    "                    weighted_return = yearly_return_value * weight\n",
    "\n",
    "                    # Update for value-weighted\n",
    "                    weighted_sum_data[center_year_decile][year_offset] += weighted_return\n",
    "                    weighted_count_data[center_year_decile][year_offset] += 1\n",
    "\n",
    "                    # Update for equal-weighted\n",
    "                    equal_weight_sum_data[center_year_decile][year_offset] += yearly_return_value\n",
    "                    equal_weight_count_data[center_year_decile][year_offset] += 1\n",
    "\n",
    "for decile in equal_weight_sum_data:\n",
    "    for year_offset in equal_weight_sum_data[decile]:\n",
    "        # Calculate equal-weighted average\n",
    "        if equal_weight_count_data[decile][year_offset] > 0:\n",
    "            equal_weight_average_data[decile][year_offset] = equal_weight_sum_data[decile][year_offset] / equal_weight_count_data[decile][year_offset]\n",
    "\n",
    "        # Calculate value-weighted average\n",
    "        if weighted_count_data[decile][year_offset] > 0:\n",
    "            value_weighted_average_data[decile][year_offset] = weighted_sum_data[decile][year_offset] / weighted_count_data[decile][year_offset]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial & Return Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging Logic\n",
    "average_data = defaultdict(float)\n",
    "sum_data = defaultdict(float)\n",
    "count_data = defaultdict(float)\n",
    "\n",
    "average_data_L2ASSETG = defaultdict(float)\n",
    "sum_data_L2ASSETG = defaultdict(float)\n",
    "count_data_L2ASSETG = defaultdict(float)\n",
    "\n",
    "average_data_MV = defaultdict(float)\n",
    "sum_data_MV = defaultdict(float)\n",
    "count_data_MV = defaultdict(float)\n",
    "\n",
    "\n",
    "for gvkey in tqdm(ASSETG_data['gvkey'].unique(), desc='Processing stocks'):\n",
    "    min_year = min(data_dict[gvkey]) + 4\n",
    "    max_year = max(data_dict[gvkey]) - 5 \n",
    "    for center_year in tqdm(range(min_year, max_year + 1), desc=f'Processing years for gvkey {gvkey}'):\n",
    "        if has_complete_data_for_period(data_dict, gvkey, center_year):\n",
    "            center_year_decile = data_dict[gvkey][center_year]['decile']\n",
    "            assetg_value = data_dict[gvkey][center_year]['ASSETG']\n",
    "            l2assetg_value = data_dict[gvkey][center_year]['L2ASSETG']\n",
    "            MV_value = data_dict[gvkey][center_year]['MV']\n",
    "\n",
    "            # Accumulate sum and count for center year\n",
    "            sum_data[center_year_decile] += assetg_value\n",
    "            count_data[center_year_decile] += 1\n",
    "\n",
    "            sum_data_L2ASSETG[center_year_decile] += l2assetg_value\n",
    "            count_data_L2ASSETG[center_year_decile] += 1\n",
    "\n",
    "            sum_data_MV[center_year_decile] += l2assetg_value\n",
    "            count_data_MV[center_year_decile] += 1\n",
    "\n",
    "        else:\n",
    "            print(f\"Incomplete data for gvkey {gvkey} in center year {center_year}\")\n",
    "\n",
    "for decile in sum_data:\n",
    "    if count_data[decile] > 0:\n",
    "        average_data[decile] = sum_data[decile] / count_data[decile]\n",
    "    if count_data_L2ASSETG[decile] > 0:\n",
    "        average_data_L2ASSETG[decile] = sum_data_L2ASSETG[decile] / count_data_L2ASSETG[decile]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Financial & Return Characteristics Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_df = []\n",
    "\n",
    "for decile in range(10):\n",
    "        row_data = [average_data[decile], average_data_L2ASSETG[decile]]\n",
    "        data_for_df.append(row_data)\n",
    "\n",
    "df = pd.DataFrame(data_for_df, columns=['Average ASSETG', 'Average L2ASSETG'], index=[f'Decile {i}' for i in range(10)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
