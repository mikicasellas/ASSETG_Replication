{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "conn = wrds.Connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATAFMT='STD' and INDFMT='INDL' and CONSOL='C' and POPSRC='D' to retrieve the standardized (as opposed to re-stated data), consolidated (as opposed to pro-forma) data presented in the industrial format (as opposed to financial services format) for domestic companys (as opposed to international firms), i.e., the U.S. and Canadian firms.\n",
    "\n",
    "PRCC_C: close market price, CSHO: net number of all common shares\n",
    "\n",
    "Importing linking table to convert permno to gvkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSETG_query = \"\"\"\n",
    "SELECT gvkey, datadate, at\n",
    "FROM comp.funda\n",
    "WHERE indfmt='INDL' \n",
    "AND datafmt='STD' \n",
    "AND popsrc='D' \n",
    "AND consol='C' \n",
    "AND datadate >= '1961-12-31' \n",
    "AND datadate <= '2002-12-31'\n",
    "\"\"\"\n",
    "June_query = \"\"\"\n",
    "SELECT permno, date, shrout\n",
    "FROM crsp.msf\n",
    "WHERE date >= '1962-06-30' -- Adjusted start date to align with ASSETG data\n",
    "AND date <= '2003-06-30' -- Adjusted end date\n",
    "AND EXTRACT(MONTH FROM date) = 6 AND EXTRACT(DAY FROM date) = 30 -- Filter for June 30th\n",
    "\"\"\"\n",
    "\n",
    "Link_query = \"\"\"\n",
    "SELECT Gvkey, Lpermno, Linkdt, Linkenddt\n",
    "FROM crsp.ccmxpf_linktable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_assetg(group):\n",
    "    # Shift within the group\n",
    "    group['at_lag3'] = group['at'].shift(3)  \n",
    "    group['at_lag2'] = group['at'].shift(2)\n",
    "    group['at_lag1'] = group['at'].shift(1)\n",
    "\n",
    "    # Calculate ASSETG\n",
    "    group['ASSETG'] = (group['at_lag1'] - group['at_lag2']) / group['at_lag2']\n",
    "    group['L2ASSETG'] = (group['at_lag2'] - group['at_lag3']) / group['at_lag3']\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab data\n",
    "ASSETG_data = conn.raw_sql(ASSETG_query)\n",
    "June_data = conn.raw_sql(June_query)\n",
    "daily_rets = pd.read_csv('daily_rets.csv')\n",
    "linking_table = conn.raw_sql(Link_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert closing price to abs, filter to only include june 30th entries, merge to shares outstanding df & calculate market value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98328/3654502307.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  daily_rets = daily_rets[daily_rets['date'].dt.month == 6][daily_rets['date'].dt.day == 30]\n"
     ]
    }
   ],
   "source": [
    "daily_rets['prc'] = daily_rets['prc'].abs()\n",
    "daily_rets['date'] = pd.to_datetime(daily_rets['date'])\n",
    "daily_rets = daily_rets[daily_rets['date'].dt.month == 6][daily_rets['date'].dt.day == 30]\n",
    "\n",
    "June_data['date'] = pd.to_datetime(June_data['date'])\n",
    "merged_data = pd.merge(June_data, daily_rets[['permno', 'date', 'prc']], \n",
    "                       on=['permno', 'date'], \n",
    "                       how='left')\n",
    "merged_data = merged_data.dropna()\n",
    "merged_data['MV'] = merged_data['shrout'] * merged_data['prc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "switches date to datetime object, creates year column, \n",
    "ensures proper sorting & filters by december month (redundent maybe),\n",
    "applies shifts and calculates ASSETG, drops na and 0 observations, creates deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSETG_data['datadate'] = pd.to_datetime(ASSETG_data['datadate'])\n",
    "ASSETG_data['year'] = ASSETG_data['datadate'].dt.year\n",
    "ASSETG_data = ASSETG_data.sort_values(['gvkey','datadate'])\n",
    "ASSETG_data = ASSETG_data[ASSETG_data['datadate'].dt.month==12]\n",
    "ASSETG_data = ASSETG_data.groupby('gvkey').apply(calculate_assetg)\n",
    "ASSETG_data = ASSETG_data[\n",
    "    (~pd.isna(ASSETG_data['at_lag1'])) & (ASSETG_data['at_lag1'] != 0) &\n",
    "    (~pd.isna(ASSETG_data['at_lag2'])) & (ASSETG_data['at_lag2'] != 0) &\n",
    "    (~pd.isna(ASSETG_data['at_lag3'])) & (ASSETG_data['at_lag3'] != 0)\n",
    "]\n",
    "ASSETG_data['decile'] = pd.qcut(ASSETG_data['ASSETG'], 10, labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging linking table to associate MV dates to ASSETG table (not done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "linking_table = linking_table.dropna(subset=['lpermno'])\n",
    "linking_table['linkdt'] = pd.to_datetime(linking_table['linkdt'])\n",
    "linking_table['linkenddt'] = pd.to_datetime(linking_table['linkenddt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSETG_data = ASSETG_data.reset_index(drop=True)\n",
    "ASSETG_data = ASSETG_data.merge(linking_table, on='gvkey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creates dictionary with this strcuture: gvkey(year(ASSETG:x, decile:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = defaultdict(lambda: defaultdict(dict))\n",
    "for idx, row in ASSETG_data.iterrows():\n",
    "    data_dict[row['gvkey']][row['year']] = {'ASSETG': row['ASSETG'], 'L2ASSETG': row['L2ASSETG'], 'decile': row['decile']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each unique gvkey: checks that iterated year has valid period, grabs decile of iterated year and for each year surrounding it grabs associated ASSETG, sums it with associated decile and offset year key, counts the amount of times its iterated, calculates averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_complete_data_for_period(data_dict, gvkey, center_year):\n",
    "    for year_offset in range(-4, 6):  # 10-year window: 4 years back, 5 years forward\n",
    "        year = center_year + year_offset\n",
    "        if year not in data_dict[gvkey]:\n",
    "            return False  # Data for this year is missing or incomplete\n",
    "    return True  # All years in the period have the necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Averaging Logic\n",
    "sum_data = defaultdict(lambda: defaultdict(int))\n",
    "count_data = defaultdict(lambda: defaultdict(int))\n",
    "average_data = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "sum_data_L2ASSETG = defaultdict(int)\n",
    "count_data_L2ASSETG = defaultdict(int)\n",
    "average_data_L2ASSETG = defaultdict(float)\n",
    "\n",
    "for gvkey in tqdm(ASSETG_data['gvkey'].unique(), desc='Processing stocks'):\n",
    "    min_year = min(data_dict[gvkey]) + 4\n",
    "    max_year = max(data_dict[gvkey]) - 5 \n",
    "    for center_year in tqdm(range(min_year, max_year + 1), desc=f'Processing years for gvkey {gvkey}'):\n",
    "        if has_complete_data_for_period(data_dict, gvkey, center_year):\n",
    "            center_year_decile = data_dict[gvkey][center_year]['decile']\n",
    "            for year_offset in range(-4, 6):\n",
    "                year = center_year + year_offset\n",
    "                if year in data_dict[gvkey]:\n",
    "                    assetg_value = data_dict[gvkey][year]['ASSETG']\n",
    "\n",
    "                    # Update sum and count\n",
    "                    sum_data[center_year_decile][year_offset] += assetg_value\n",
    "                    count_data[center_year_decile][year_offset] += 1\n",
    "\n",
    "                    # Calculate and update the average\n",
    "                    current_sum = sum_data[center_year_decile][year_offset]\n",
    "                    current_count = count_data[center_year_decile][year_offset]\n",
    "                    average_data[center_year_decile][year_offset] = current_sum / current_count if current_count != 0 else 0\n",
    "                    \n",
    "                    if year_offset == 0:\n",
    "                        l2assetg_value = data_dict[gvkey][year]['L2ASSETG']\n",
    "                        sum_data_L2ASSETG[center_year_decile] += l2assetg_value\n",
    "                        count_data_L2ASSETG[center_year_decile] += 1\n",
    "\n",
    "        else:\n",
    "            print(f\"Incomplete data for gvkey {gvkey} in center year {center_year}\")\n",
    "\n",
    "for decile in sum_data_L2ASSETG:\n",
    "    if count_data_L2ASSETG[decile] > 0:\n",
    "        average_data_L2ASSETG[decile] = sum_data_L2ASSETG[decile] / count_data_L2ASSETG[decile]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates ASSETG panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_df = {}\n",
    "for year_offset in range(-4, 6):\n",
    "    row_data = []\n",
    "    for decile in range(10):\n",
    "            row_data.append(average_data[decile][year_offset])\n",
    "    data_for_df[year_offset] = row_data\n",
    "\n",
    "df = pd.DataFrame(data_for_df, index=range(10)).transpose()\n",
    "df.columns = [f'Decile {i}' for i in range(10)]\n",
    "df.index.name = 'Year Offset'\n",
    "df['9-0 Spread'] = df['Decile 9'] - df['Decile 0']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
